<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://updates.cardano.intersectmbo.org/reports</id>
    <title>Cardano Development Updates Blog</title>
    <updated>2026-02-19T11:24:55.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://updates.cardano.intersectmbo.org/reports"/>
    <subtitle>Cardano Development Updates Blog</subtitle>
    <icon>https://updates.cardano.intersectmbo.org/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.6.2]]></title>
        <id>2026-02-performance-10.6.2</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2026-02-performance-10.6.2"/>
        <updated>2026-02-19T11:24:55.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.5.4</code> - the current Node 10.5 Mainnet release</li><li><code>10.6.2</code> - the current Node 10.6 Mainnet release</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each txn consumes 2 inputs and creates 2 outputs, changing the UTxO set. Full blocks (&gt; 80kB) exclusively; high submission pressure (TPS &gt; 10).</li><li><em>Plutus</em>: Each txn contains a Plutus script exhausting the per-tx execution budget. Small blocks (&lt; 3kB) exclusively; low submission pressure (TPS &lt; 1).</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.6.2</code> exhibits a clear 6% <em>reduction</em> in Process CPU usage under full saturation workload, and a 2% <em>reduction</em> under Plutus workload.</li><li>Allocation rate and Minor GCs are significantly <em>reduced</em> as well (~59% and 64% depending on workload).</li><li>Major GC events <em>go up</em> by 27% and 34%, depending on workload.</li><li>Observed CPU 85% spans exhibit a clear <em>increase</em> in duration -- ~4.1 and ~1.8 slots depending on workload.</li><li>RAM usage decreases by 19% and 24% depending on workload. <strong>HOWEVER</strong>: This is a known result of optimizations in the benchmarking setup. From a seperate benchmark with those optimizations applied on top of <code>10.5</code>, we know that <code>10.6.2</code> exhibits a very minor <em>increase</em> (1% - 2%) in average Heap size, with a <em>reduction</em> in maximum Heap size under saturation workload only.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>We can observe Ledger Ticking <em>decrease</em> by 3ms - 5ms.</li><li>Under saturation workload only, Mempool snapshotting and Forged to Sending exhibit small <em>increases</em> by 2ms each.</li><li>As a result, a block producer is able to announce a new header 2ms - 3ms <em>earlier</em> into a slot (depending on workload).</li><li>Self adoption on the forging node also <em>decreases</em> by 2ms - 3ms.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration <em>decreases</em> by 2% (3ms or 5ms, depending on workload).</li><li>For small blocks, Adoption times on the peers <em>increase</em> by 2ms, however, for large blocks they <em>decrease</em> by 3ms.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Cluster adoption metrics on <code>10.6.2</code> exhibit no significant change under high submission / large blocks workload.</li><li>Under low submission / small blocks workload, there are <em>small improvements</em> of 1% - 3% in the body of the distribution, and a 10% <em>increase</em> in the 100th centile.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li><code>10.6.2</code> is more efficient in its usage of CPU time. Considered in conjunction with the increase in CPU 85% spans, it points to a redistribution of work which results in less bursts, and more plateaus.</li><li>Seeing there is no negative impact on block production and diffusion metrics, and considering the overall decrease in CPU usage, the CPU 85% span increase can't be interpreted as a risk to performance or responsiveneess.</li><li>The RAM increase observed on <code>10.6.0-pre</code> has been successfully fixed in <code>10.6.2</code>.</li><li><code>10.6.2</code> is more efficient wrt. block production and diffusion.</li><li>Adoption metrics are largely equivalent to <code>10.5.4</code>; the 10% increase in the Plutus workload's 100th centile is an outlier resulting from the benchmark's very restrictive topology. Unless the increase also manifests in the 98th and 96th centiles, or below, it is not considered a risk.</li><li>From a performance perspective, we can determine <code>10.6.2</code> to be regression-free and attest a clean bill of health.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full comparison for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.6.2.value-only-4c5125d828beea03ce5c8a93e305f01a.pdf">here</a>.</p><p>Full comparison for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.6.2.plutus-bc20f4b653dfa535cbe146fc50eef62d.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.5.4]]></title>
        <id>2026-02-performance-10.5.4</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2026-02-performance-10.5.4"/>
        <updated>2026-02-09T17:19:04.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.5-baseline</code> - performance baseline from previous Node 10.5 releases</li><li><code>10.5.4</code> - the current Node 10.5.4 release</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.5.4</code> exhibits a slight 3% <em>increase</em> in Process CPU usage -- regardless of workload type.</li><li>RAM usage <em>decreases</em> slightly by 3% (0.23GiB - 0.26GiB, depending on workload).</li><li>Observed CPU 85% span duration exhibits a very faint <em>increase</em> -- between ~0.25 and ~0.35 slots.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>For large blocks only, we can observe a small <em>increase</em> in Adoption time by 2ms.</li><li>Beyond that, there are no significant changes to block production metrics.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>For large blocks, average Block Fetch duration <em>decreases</em> by 2ms, but Adoption times on the peers <em>increase</em> by 2ms.</li><li>For small blocks, average Block Fetch duration <em>increases</em> by 5ms, but Adoption times on the peers <em>decrease</em> by 3ms.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Cluster adoption metrics on <code>10.5.4</code> exhibit no significant change under high submission / large blocks workload.</li><li>Under Plutus workload (low submission / small blocks), there's a minor <em>increase</em> by 2% - 3% between the 80th and 98th centiles.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>The small up and down changes in Adoption times and Block fetch duration are well within margin of slack and do not pose any kind of performance risk.</li><li>Transitively, this applies to observed E2E propagation metrics as well.</li><li>All in all, <code>10.5.4</code> is pretty closely aligned with the existing 10.5 performance baseline.</li><li>From a performance perspective, we can determine it to be regression-free and attest a clean bill of health.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full comparison for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.5.4.value-only-e89ab94a155f9b98f7dc89236c1c9cc0.pdf">here</a>.</p><p>Full comparison for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.5.4.plutus-ce49bd3a23c4969dad76dadbbfa9ff76.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory Budget Scaling -- 10.6]]></title>
        <id>2026-01-execbudget-memory-10.6</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2026-01-execbudget-memory-10.6"/>
        <updated>2026-01-20T11:22:43.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>This report compares benchmarking runs for 3 different settings of the Plutus memory execution budget:</p><ul><li><code>10.6.1-jan26</code> - current mainnet memory execution budget</li><li><code>mem-x1.5</code> - 1.5 x current mainnet memory execution budget</li><li><code>mem-x2</code> - 2 x current mainnet memory execution budget</li></ul><p>For this comparison, we gather various metrics under the <em>Plutus</em> workload used in release benchmarks: Each block produced during the benchmark contains
4 identical script transactions calibrated to fully exhaust the memory execution budget. Thus, script execution is constrained by the memory budget limit
every case. The workload produces small blocks (&lt; 3kB) exclusively.  </p><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. </p><p>Identical scaling benchmarks were performed in Q1 2025 on <a href="https://updates.cardano.intersectmbo.org/reports/2025-03-execbudget-memory-10.2" target="_blank" rel="noopener noreferrer">Node 10.2 / GHC8.10</a> and <a href="https://updates.cardano.intersectmbo.org/reports/2025-05-execbudget-memory-10.3" target="_blank" rel="noopener noreferrer">Node 10.3 / GHC9.6</a>. This comparison aims to ascertain the past observations and conclusions still apply,
given the most recent Node version (10.6.1, with patches for 10.6.2 backported) and its recommended compiler version GHC9.6.7.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget impacts Allocation Rate and Minor GCs. 1.5 x the budget results in rises of 21% - 22%; for doubling the budget the corresponding rises are 36% - 38%.</li><li>These increases exhibit a slightly sublinear correlation with raising mem budget; however, in absolute terms they are much steeper (roughly 4x) compared to Node 10.3.</li><li>The Node process RAM footprint is unaffected by and the effects on Process CPU usage is negligible for either scaling factor.</li><li>CPU 85% span duration exhibits a slight constant increase (approx. 0.5 slots) when scaling the mem budget, regardless of the factor.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget has significant impact on self adoption time only.</li><li>Scaling by factor 1.5 leads to an 11ms (or 25%) increase, whereas factor 2 leads to 22ms (51%); this is fully congruent with the observations on Node 10.3.</li><li>This increase is linearly correlated with raising the mem budget.</li><li>With increased memory budget, counterintuitively, the time from slot start until new header announcement <em>decreases</em> slightly - by 4ms (factor 1.5) and 2ms (factor 2). This was not observed on Node 10.3.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Same as on the block producer, scaling the memory budget has significant impact on block adoption times only.</li><li>Scaling by factor 1.5 leads to an 10ms (or 22%) increase, whereas factor 2 leads to 17ms (37%).</li><li>Again, these increases exhibit a slightly sublinear correlation with raising the mem budget - largely congruent with the absolute increases seen on Node 10.3.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>1.5 x the memory budget results in a minor increase of 2ms - 16ms in cluster adoption times (1% - 3%).</li><li>2 x the memory budget results in a slight 10ms - 13ms increase (2% - 3%).</li><li>These increases are clearly lower (very roughly 2.5x) than those observed on Node 10.3.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><p>These measurements outline the headroom for raising the memory budget, along with the expected performance impact:</p><ol><li>Block adoption time is the only network metric that's affected significantly, increasing both on the forger and the peers by the same extent.</li><li>These increases seem to correspond linearly at worst with raising the memory budget. This gives excellent predictability of performance impact up to a hypothetical 100% raise.</li><li>Expectedly, more allocations and minor GCs take place; however, CPU and RAM usage remain nearly constant.</li><li>Block diffusion is only marginally affected by changing the execution budget: Due to header pipelining, announcing and (re-)sending a block precedes adoption in most cases.</li><li>As such, measurements taken with either budget adjustment <em>do not indicate performance risks</em> to the network, but clearly evidence their respective performance cost.</li><li>With the exception of extra allocations, all measurements point to a recent Node version delivering equal or slightly better performance compared to 10.3 given some memory budget increase.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachment">Attachment<a class="hash-link" href="#attachment" title="Direct link to heading">​</a></h2><p>Full report PDF downloadable <a target="_blank" href="/assets/files/execbudget-10.6-mem_scaling-ab738e4235d68f34ae678fbff73c89dc.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.6.0-pre]]></title>
        <id>2025-11-performance-10.6.0-pre</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-11-performance-10.6.0-pre"/>
        <updated>2025-11-21T10:36:00.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.5</code> - baseline from the previous Node release</li><li><code>10.6.0-pre</code> - the current (pre-)release tag</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.6.0-pre</code> exhibits a slight shift in CPU usage. It consumes 3% <em>less</em> CPU time under saturation, whereas under a low submission workload it consumes 4% <em>more</em>.</li><li>Allocation rate and Minor GCs impact are significantly <em>reduced</em> - (~45% and ~59% depending on workload). This takes much pressure away from the garbage collector.</li><li>RAM usage <em>increases</em> significantly by 0.9GiB - 1.1GiB (15% - 17% depending on workload).</li><li>Observed CPU 85% spans are <em>longer</em> -- ~3.1 slots under value and ~1.6 slots under Plutus workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>We can observe slight <em>increases</em> in Ledger Ticking and Mempool Snapshotting by 2ms each.</li><li>This causes a block producer to announce a new header 3ms - 4ms (or 6% - 12%) later into a slot.</li><li>Additionally, Adoption time on the block producer also increases by 3ms - 4ms.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under saturation workload only, Block Fetch duration increases by 7ms (or 2%).</li><li>Adoption times on the peers increase by 2ms - 3ms.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Cluster adoption metrics on <code>10.6.0-pre</code> exhibit a small 2% - 3% increase across all centiles.</li><li>Under Plutus workload only, the increase becomes superlinear in 98th-100th centiles, with an extra 89ms (or 18%) required for full cluster adoption.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>The small increases in block production, diffusion and adoption metrics do not pose a performance risk to the network.</li><li>The restricted topology of the benchmark forces a regression in the tail end of the adoption metrics distribution to surface; in a live network, this is mitigated by a much higher number of connected peers / peer sharing.</li><li>The increase in RAM usage has so far not manifested on relay nodes deployed in a live network; as this is a pre-relase, the precise effect of our benchmark (exposing block producers to high pressure over extended time) is under investigation.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full comparison for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.6.0-pre.value-only-d5ecc8e6d75d5fee270457bb1ca04560.pdf">here</a>.</p><p>Full comparison for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.6.0-pre.plutus-1cf7dc50db2395494d4e3295fc7fbc61.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.5.0]]></title>
        <id>2025-07-performance-10.5.0</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-07-performance-10.5.0"/>
        <updated>2025-07-02T09:50:10.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.4.1</code> - baseline from the previous Node release</li><li><code>10.5.0</code> - the current (pre-)release tag</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="preliminaries">Preliminaries<a class="hash-link" href="#preliminaries" title="Direct link to heading">​</a></h2><ol><li>The feature in <code>10.5</code> with major performance impact is periodic <a href="https://github.com/IntersectMBO/cardano-node/pull/6180" target="_blank" rel="noopener noreferrer">ledger metrics</a>. This is exclusive to the <a href="https://developers.cardano.org/docs/get-started/cardano-node/new-tracing-system/new-tracing-system" target="_blank" rel="noopener noreferrer">new tracing system</a>.</li><li><code>10.5</code> flips the default config for <code>PeerSharing</code> to <code>true</code>; however, the recommendation is to explicitly set it to <code>false</code> on block producers. If not for privacy issues alone, we also found disadvantageous performance impact on block production when enabled. Hence, our benchmarks do not factor in that overhead.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.5.0</code> shows a clear reduction in CPU usage - by ~30% regardless of workload type.</li><li>Furthermore, Allocation rate and GC impact are clearly reduced - by 27%-29% and 24%-25% respectively.</li><li>Heap size <em>increases</em> very slightly under saturation (by 1%) and <em>decreases</em> very slightly (by 1%) under Plutus workload.</li><li>CPU 85% spans are slightly <em>shorter</em> (~0.2 slots) under saturation, and slightly <em>longer</em> (~0.26 slots) under Plutus workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Block Context Acquisition time (prior to leadership check) is greatly reduced - from ~24ms to under 1ms.</li><li>Under saturation only, Ledger Ticking and Mempool Snapshotting exhibit very slight upticks (by 3ms and 2ms respectively).</li><li>Under Plutus workload only, Self Adoption on the forger exhibits a very slight uptick (by 3ms).</li><li>In summary, a block producer is able to announce a new header 20ms or 21% earlier into the slot (22ms or 43% under Plutus workload).</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under saturation workload only, Block Fetch duration increases by 14ms (or 4%).</li><li>Under saturation, block adoption is slightly <em>faster</em> (by 3ms), while under Plutus workload it's slightly <em>slower</em> (by 2ms).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under saturation workload, cluster adoption times on <code>10.5.0</code> are identical to those on <code>10.4.1</code>.</li><li>Under Plutus workload, they show a moderate 3% - 5% <em>improvement</em>, with 7% in the 50th percentile.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>We could not detect any regressions or performance risks to the network on <code>10.5.0</code>.</li><li>CPU usage is clearly reduced.</li><li>The forging loop executes faster, new header announcements happen earlier.</li><li>Diffusion / adoption metrics exhibit a small overall improvement and indicate <code>10.5.0</code> will deliver network performance at least comparable to <code>10.4.1</code>.</li><li>All improvements listed above hinge on the <a href="https://github.com/IntersectMBO/cardano-node/pull/6180" target="_blank" rel="noopener noreferrer">ledger metrics</a> feature and will materialize only when using the <a href="https://developers.cardano.org/docs/get-started/cardano-node/new-tracing-system/new-tracing-system" target="_blank" rel="noopener noreferrer">new tracing system</a>. Using the legacy system, <code>10.5.0</code> performance is expected to be almost identical to <code>10.4.1</code>.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full comparison for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.5.0.value-only-c886a172ada813c722a3e366f9a8ff46.pdf">here</a>.</p><p>Full comparison for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.5.0.plutus-8be2a58dd7bc67d41af6254b47ce4a58.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.5.0</code> extend to a potential <code>10.5.1</code> tag, as that won't include any changes with a performance impact; thus, measurements performed on <code>10.5.0</code> remain valid.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.4.1]]></title>
        <id>2025-05-performance-10.4.1</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-05-performance-10.4.1"/>
        <updated>2025-05-05T15:29:39.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.3.1</code> - baseline from the previous Node release</li><li><code>10.4.1</code> - the current release</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><p><code>10.4.1</code> features the UTxO-HD in-memory backing store <code>V2InMemory</code> of <code>LedgerDB</code>, which replaces the in-memory representation of UTxO entries in <code>10.3</code> and prior.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>On <code>10.4.0</code> under value workload, Heap size increases slightly by 2%, and 5% under Plutus workload. This corresponds to using ~170MiB-390MiB additional RAM.</li><li>Allocation rate and GC impact are virtually unchanged.</li><li>Process CPU usage improves slightly by 2% regardless of workload type.</li><li>CPU 85% spans are slightly (~0.37 slots) <em>longer</em> under value workload, and slightly <em>shorter</em> (~0.33) under Plutus workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>We can observe a clear improvement in Mempool snapshotting by 9ms or 16% (2ms or 8% under Plutus workload).</li><li>Self-Adoption time improves by 4ms or 5% (and remains virtually unchanged under Plutus workload).</li><li>Hence a block producer is able to announce a new header 10ms or 9% earlier into the slot (1ms or 2% under Plutus workload).</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under value workload, Fetch duration and Fetched to Sending improve slightly by 3ms (1%) and 2ms (4%).</li><li>Under Plutus workload, Fetched to Sending has a slightly longer delay - 2ms (or 5%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload, cluster adoption times exhibit a small 1% - 3% <em>improvement</em> across all percentiles.</li><li>Under Plutus workload, they show a small 1% - 2% <em>increase</em> across all percentiles (except the 80th).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>We could not detect any regressions or performance risks to the network on <code>10.4.1</code>.</li><li>There is a small and reasonable price to pay in RAM usage for adding the <code>LedgerDB</code> abstraction and thus enable exchangeable backing store implementations.</li><li>On the other hand, CPU usage is reduced slightly by use of the in-memory backing store.</li><li><code>10.4.1</code> is beneficial in all cases for block production metrics; specifically, block producers will be able to announce new headers earlier into the slot.</li><li>Network diffusion and adoption metrics vary only slightly and indicate <code>10.4.1</code> will deliver network performance comparable to <code>10.3.1</code>.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full comparison for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.4.1.value-only-d204944948caf9a791ebe994a60cc9dd.pdf">here</a>.</p><p>Full comparison for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.4.1.plutus-1cf67914451f54090b7441b69fb5579a.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.4.1</code> were performed on tag <code>10.4.0</code>. The patch version bump did not include changes relevant to performance; thus, measurements performed on <code>10.4.0</code> remain valid. The same holds for <code>10.3.1</code> and <code>10.3.0</code>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.3.1]]></title>
        <id>2025-04-performance-10.3.1</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-04-performance-10.3.1"/>
        <updated>2025-04-22T13:54:50.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 3 different versions of <code>cardano-node</code>:</p><ul><li><code>10.2</code> - baseline from the previous release (bulit with GHC8.10.7)</li><li><code>10.3.0-ghc8107</code> - the current release built with GHC8.10.7</li><li><code>10.3.0-ghc965</code> - the current release built with GHC9.6.5</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><p><code>10.3.1</code> supports two compiler versions, which will be taken into account when comparing performance of different builds of that release.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.3.1</code> exhibits a clear reduction in Process CPU usage, more prominent under <em>value workload</em>:<ul><li>value workload: 10% with GHC8, and 24% with GHC9.</li><li>Plutus workload: 4% GHC8, and 6% with GHC9.</li></ul></li><li>There also is a reduction in RAM usage, more prominent under <em>Plutus workload</em>:<ul><li>value workload: 1% or ~54MiB with GHC8, and 6% or ~574MiB with GHC9.</li><li>Plutus workload: 14% or ~1.2GiB with GHC9 only.</li></ul></li><li>Minor GCs and Allocation rate both drop on <code>10.3.1</code>, more significantly under <em>value workload</em>:<ul><li>value workload: 11% each with GHC8, and 24% each with GHC9.</li><li>Plutus workload: 3% and 1% with GHC8; 5% and 4% with GHC9. </li></ul></li><li>Under value workload, CPU 85% spans <em>increase</em> by 45% with GHC8, but only by 14% with GHC9.</li><li>Under Plutus workload, those spans <em>decrease</em> by 5% with GHC8; even by 19% with GHC9.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Under value workload, several block production metrics improve clearly on <code>10.3.1</code>, most prominently Mempool Snapshotting.</li><li>With GHC8, the improvement is 23%, with further significant improvements in Adoption time (11%) and Ledger ticking (10%).</li><li>With GHC9, the improvement is 27%, with further significant improvements in Adoption time (10%) and Ledger ticking (17%).</li><li>Under value workload, this enables a block producer to announce a header earlier into the slot, namely by 23ms (GHC8) and by 28ms (GHC9).</li><li>Under Plutus workload, Adoption time <em>increases</em> by 3ms (6%) with GHC8, but <em>decreases</em> by 8ms (15%) with GHC9.</li><li>Furthermore, there are no significant changes to the header announcement timing.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under value-only workload only, we observe an increase in Block Fetch duration: 7ms (2%) with GHC8, and 23ms (7%) with GHC9.</li><li>Block adoption times on the peers improve clearly: 11ms (12%) with GHC8, and 12ms (14%) with GHC9.</li><li>Under Plutus workload, however, similarly to the block producer, adoption times <em>increase</em> by 3ms (6%) with GHC8, but <em>decrease</em> by 7ms (13%) with GHC9.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload, cluster adoption times on <code>10.3.1</code> are largely unchanged.</li><li>With GHC8, there are 5% and 3% improvements in the 50th and 100th centiles; with GHC9, there's a small 3% improvement in the 50th centile.</li><li>Under Plutus workload, with GHC8, there's a moderate 6% <em>increase</em> in cluster adoption times in the 100th centile.</li><li>With GHC9, however, there's a small 2% <em>improvement</em> in all but the 100th centile.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>For <code>10.3.1</code> we could not detect any performance risks or regressions.</li><li>Improving resource usage was a stated goal for the <code>10.3</code> release; this could be confirmed via measurements for CPU and RAM usage as well as CPU spikes.</li><li><code>10.3.1</code> achieves network performance comparable to <code>10.2.1</code> using clearly less system resources - for both compiler versions.</li><li>Several key metrics improve on <code>10.3.1</code>: Block producers announce a new header sooner into the slot; we observe lower adoption times (GHC9 only).</li><li>The GHC9.6.5 build has demonstrable performance advantages over the GHC8.10.7 build; especially the Plutus interpreter seems to gain considerably from using GHC9. For those reasons we now recommend <em>GHC9.6.x</em> for production builds.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.3.1.value-only-cfaa4837c218e103c2b151a4bd1a030a.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.3.1.plutus-45a3a1b97e744ee88194002a805d665a.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.3.1</code> were performed on tag <code>10.3.0</code>. The patch version bump did not include changes relevant to performance; thus, measurements performed on <code>10.3.0</code> remain valid.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory Budget Scaling -- 10.2]]></title>
        <id>2025-03-execbudget-memory-10.2</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-03-execbudget-memory-10.2"/>
        <updated>2025-04-01T09:50:10.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>This report compares benchmarking runs for 3 different settings of the Plutus memory execution budget:</p><ul><li><code>loop-memx1</code> - current mainnet memory execution budget</li><li><code>loop-memx1.5</code> - 1.5 x current mainnet memory execution budget</li><li><code>loop-memx2</code> - 2 x current mainnet memory execution budget</li></ul><p>For this comparison, we gather various metrics under the <em>Plutus</em> workload used in release benchmarks: Each block produced during the benchmark contains
4 identical script transactions calibrated to fully exhaust the memory execution budget. Thus, script execution is constrained by the memory budget limit
every case. The workload produces small blocks (&lt; 3kB) exclusively.  </p><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. Node
version 10.2 was used, built with GHC8.10.7.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget impacts Allocation Rate and Minor GCs. 1.5 x the budget results in rises of 5% and 6% respecetively; for doubling the budget the corresponding rises are 10% and 11%.</li><li>Those increases seem to correlate linearly with raising mem budget.</li><li>The effect on CPU usage is almost negligible: a 1% (or 3%, for doubling the budget) increase of Process CPU.</li><li>The Node process RAM footprint is unaffected.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget has significant impact on block adoption time only.</li><li>Scaling by factor 1.5 leads to a 14ms (or 25%) increase, whereas factor 2 leads to 28ms (49%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Same as on the block producer, scaling the memory budget has significant impact on block adoption times only.</li><li>Scaling by factor 1.5 leads to a 15ms (or 26%) increase, whereas factor 2 leads to 28ms (48%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>1.5 x the memory budget results in a slight increase of 19ms - 22ms in cluster adoption times (4% - 5%).</li><li>2 x the memory budget results in a moderate 27ms - 34ms increase (5% - 7%, with 9% in the 50th centile).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><p>These measurements outline the headroom for raising the memory budget, along with the expected performance impact:</p><ol><li>Block adoption time is the only metric that's affected significantly, increasing both on the forger and the peers by the same extent.</li><li>These increases seem to correspond linearly with the raising the memory budget. This gives excellent predictability of performance impact.</li><li>Expectedly, more allocations happen; we can observe the same linear correspondence here as well.</li><li>It has to be pointed out that block diffusion is only slightly affected by changing the execution budget: Due to pipelining, announcing and (re-)sending a block precedes adoption in most cases.</li><li>As such, regarding absolute cluster adoption times, measurements taken with either budget adjustment do not exhibit performance risks to the network. They do illustrate, however, the performance cost of those budget adjustments.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachment">Attachment<a class="hash-link" href="#attachment" title="Direct link to heading">​</a></h2><p>Full report PDF downloadable <a target="_blank" href="/assets/files/execbudget-10.2-mem_scaling-ae344917bf4358c013821b4a7449f283.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory Budget Scaling -- 10.3]]></title>
        <id>2025-05-execbudget-memory-10.3</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-05-execbudget-memory-10.3"/>
        <updated>2025-04-01T09:50:10.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>This report compares benchmarking runs for 3 different settings of the Plutus memory execution budget:</p><ul><li><code>10.3-ghc965</code> - current mainnet memory execution budget</li><li><code>loop-memx1.5</code> - 1.5 x current mainnet memory execution budget</li><li><code>loop-memx2</code> - 2 x current mainnet memory execution budget</li></ul><p>For this comparison, we gather various metrics under the <em>Plutus</em> workload used in release benchmarks: Each block produced during the benchmark contains
4 identical script transactions calibrated to fully exhaust the memory execution budget. Thus, script execution is constrained by the memory budget limit
every case. The workload produces small blocks (&lt; 3kB) exclusively. </p><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. Node
version 10.3 was used, built with GHC9.6.5. This is a re-run of the scaling benchmarks performed on Node version 10.2 / GHC8.10 to document impact of performance improvements. Those results
were published here on <a href="https://updates.cardano.intersectmbo.org/reports/2025-03-execbudget-memory-10.2" target="_blank" rel="noopener noreferrer">Cardano Updates</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget impacts Allocation Rate and Minor GCs. 1.5 x the budget results in rises of 5% each; for doubling the budget the corresponding rises are 8% and 9%.</li><li>Those increases seem to correlate linearly with raising mem budget.</li><li>The effects on CPU usage and RAM footprint are negligible for both scaling factors.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget has significant impact on block adoption time only.</li><li>Scaling by factor 1.5 leads to a 10ms (or 24%) increase, whereas factor 2 leads to 21ms (50%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Same as on the block producer, scaling the memory budget has significant impact on block adoption times only.</li><li>Scaling by factor 1.5 leads to a 11ms (or 24%) increase, whereas factor 2 leads to 19ms (42%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>1.5 x the memory budget results in a slight increase of  9ms - 31ms in cluster adoption times (3% - 6%).</li><li>2 x the memory budget results in a moderate 17ms - 35ms increase (5% - 7%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><p>These measurements outline the headroom for raising the memory budget, along with the expected performance impact:</p><ol><li>Block adoption time is the only metric that's affected significantly, increasing both on the forger and the peers by the same extent.</li><li>These increases seem to correspond linearly with the raising the memory budget. This gives excellent predictability of performance impact.</li><li>Expectedly, more allocations happen; we can observe the same linear correspondence here as well.</li><li>It has to be pointed out that block diffusion is only slightly affected by changing the execution budget: Due to pipelining, announcing and (re-)sending a block precedes adoption in most cases.</li><li>As such, regarding absolute cluster adoption times, measurements taken with either budget adjustment do not exhibit performance risks to the network. They do illustrate, however, the performance cost of those budget adjustments.</li></ol><p>These scaling benchmarks are complementary to those performed on Node 10.2; in comparison with those, we can additionally conclude:</p><ol><li>The conclusions from measurements for each scaling run set are identical.</li><li>While the relative increases in adoption time for both Node builds are quite similar, the absolute increases are 22% - 32% <em>smaller</em> (i.e., adoption happens more efficiently) for 10.3 / GHC9.6.</li><li>The same rationale applies to end-to-end propagation metrics: Absolute values document faster cluster adoption for 10.3 / GHC9.6. </li><li>Incidentally, the absolute values for scaling factor 1 on 10.2 are close to those for scaling factor 2 on 10.3 except for the tail end (i.e. 95th percentile and above).</li><li>This reflects the performance improvements that were a stated goal for the 10.3 release - and suggests the performance cost of memory budget increases has become slightly <em>smaller</em> in absolute terms.</li></ol><p>As adoption times are not only impacted by Plutus execution alone, we still advocate for a conservative and/or multi-stage raise; future backpedaling on budget limits could cause issues for scripts already deployed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachment">Attachment<a class="hash-link" href="#attachment" title="Direct link to heading">​</a></h2><p>Full report PDF downloadable <a target="_blank" href="/assets/files/execbudget-10.3-mem_scaling-af25748ab9933bb24a20c245b885dc7e.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- UTxO-HD on 10.2]]></title>
        <id>2025-02-performance-utxohd-10.2</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-02-performance-utxohd-10.2"/>
        <updated>2025-02-21T17:25:57.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>This report compares benchmarking runs for 2 different flavours of <code>cardano-node</code>:</p><ul><li><code>10.2-regular</code> - regular Node performance baseline from the <code>10.2.x</code> release benchmarks.</li><li><code>10.2-utxohd</code> - the UTxO-HD build of the Node based on that same version.</li></ul><p>For this benchmark, we're gathering various metrics under the <em>value-only</em> workload used in release benchmarks: Each transaction consumes 2 inputs and creates 2 outputs,
changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively. Moreover, it's the workload that produces most stress on the UTxO set. Thus, it's the most meaningful
workload when it comes to benchmarking UTxO-HD.  </p><p>We target the <em>in-memory backing store</em> of UTxO-HD - LedgerDB V2 in this case. The on-disk backend is not used. </p><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>With UTxO-HD's in-memory backend, the memory footprint increases slightly by 3%.</li><li>Process CPU usage is moderately reduced by 9% with UTxO-HD.</li><li>Additionally, CPU 85% spans decrease in duration by 24% (~1.1 slots).</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Block context acquisition improves by 3ms (or 11%), while Ledger ticking takes 3ms (or 10%) longer.</li><li>Creating a mempool snapshot is significantly faster - by 16ms (or 21%).</li><li>As a result, a UTxO-HD block producing node is able to announce a new header 17ms (or 12%) earlier into a slot.</li><li>Additionally, adoption time on the forger is slightly improved - by 4ms (or 5%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block fetch duration increases moderately by 13ms or 4%.</li><li>Adoption times on the peers improve very slightly - by 2ms or 2%.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>There is no significant difference in cluster adoption times between regular and UTxO-HD node.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><p>Regarding the UTxO-HD build using the in-memory LedgerDB V2 backend, we can conclude that:</p><ol><li>it is lighter on CPU usage compared to the regular node, albeit requiring just slightly more RAM.</li><li>it poses no performance risk to block producers; on the contrary, the changes in forging loop metrics seem favourable compared to the regular node.  </li><li>network performance would be expeceted to be on par with the regular node.</li><li>even under stress, there is no measurable performance regression compared to the regular node.</li><li>as a consequence of the above, performance-wise, it's a viable replacement for the regular in-memory solution.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachment">Attachment<a class="hash-link" href="#attachment" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/utxohd-10.2.value-only-6fc915d9fd584640513356bc53e858ea.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.2.1]]></title>
        <id>2025-02-performance-10.2.1</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-02-performance-10.2.1"/>
        <updated>2025-02-21T13:39:30.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.1.4</code> - baseline from a previous mainnet release</li><li><code>10.2.1</code> - the current release</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>CPU usage increases moderately by 12% under value, and very slightly by 2% under Plutus workload.</li><li>CPU 85% spans increase by 14% (~0.6 slots) under value workload, but decrease by 6% (~0.8 slots) under Plutus workload.</li><li>Only under value workload, we observe a slight increase in Allocation rate and Minor GCs of 9% and 8%</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Adoption time on the forger improves by 3ms (or 4%) - and 5ms (or 9%) under Plutus workload.</li><li>Block context acquisition takes 3ms (or 12%) longer under value workload.</li><li>Under Plutus workload only, ledger ticking improves by 3ms (or 12%).</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block fetch duration improves clearly by 16ms (or 4%) under value-only workload.</li><li>Under Plutus workload, we can measure an improvement by 4ms (or 7%) for adoption times on the peers.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><p>As a result of the above, on <code>10.2.1</code> exhibits:</p><ol><li>a slight 3% improvement in cluster adoption times in the 80th centile and above under value workload.</li><li>a near-jitter 1% - 3% improvement in cluster adoption times under Plutus workload.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>We could not detect any significant regressions, or performance risks, on <code>10.2.1</code>.</li><li><code>10.2.1</code> comes with slightly increased CPU usage, and no changes to RAM footprint.</li><li>Diffusion metrics very slightly improve - mainly due to block fetch being more efficient for full blocks, and adoption for blocks exclusively containing Plutus transactions.</li><li>This points to network performance of <code>10.2.1</code> being on par with or very slightly better than <code>10.1.4</code>.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.2.1.value-only-5cf6817ff642bf5a9fb8e80e4acbbc1c.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.2.1.plutus-253a36c1df440f4c9b5a3041ed8640e3.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.2.1</code> were performed on tag <code>10.2.0</code>. The patch version bump did not include changes relevant to performance; thus, measurements and observations performed on <code>10.2.0</code> remain valid.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.1.4]]></title>
        <id>2025-01-performance-10.1.4</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2025-01-performance-10.1.4"/>
        <updated>2025-01-10T12:16:13.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.1.1</code> - baseline from a previous mainnet release</li><li><code>10.1.4</code> - the current mainnet release</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>CPU 85% spans slightly increase by 6% or ~0.2 slots (26% or ~2.9 slots under Plutus workload).</li><li>We can observe a tiny increase in memory usage by 1-2% (132-160 MiB).</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Under value workload, Ledger Ticking and Self Adoption exhibit a very slight increase (2ms each).</li><li>Block Context Acquisition has improved by 2ms.</li><li>Under Plutus workload, there are no significant changes to forger metrics.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>There's a minor increase of 1% (3ms) in Block Fetch duration under value workload only.</li><li>Under Plutus workload, we can measure a small improvement by 2% for adoption times on the peers.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><p>As a result of the above, on <code>10.1.4</code> we can observe:</p><ol><li>a tiny increase in cluster adoption times of 1%-2% in the 80th centile and above under value workload.</li><li>an improvement in cluster adoption times of 3%-4% in the tail end (95th centile and above) under Plutus workload.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>For <code>10.1.4</code>, we could not detect any regressions or performance risks.</li><li>All increases or decreases in forger and peer metrics are 3ms and less. This indicates network performance of <code>10.1.4</code> will very closely match that of <code>10.1.1</code> and subsequent patch releases.</li><li>There's no significant change in the resource usage pattern. The increased CPU 85% spans tend to barely manifest when the system is under heavy load (value workload); as such, they pose no cause for concern. </li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.4.value-only-7e81a4debde7bea0f719279ffb8ebd51.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.4.plutus-6272430f9ef6c8d53cfeaacced5bf04c.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.1.1</code> were performed on tag <code>10.0.0-pre</code>. The minor version bump did not include changes relevant to performance; thus, measurements taken on <code>10.0.0-pre</code> remain a valid baseline.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 10.1.1]]></title>
        <id>2024-10-performance-10.1.1</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2024-10-performance-10.1.1"/>
        <updated>2024-10-31T11:03:27.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>9.2.0</code> - baseline from a previous mainnet release</li><li><code>10.1.1</code> - the current mainnet release</li></ul><p>For this benchmark, we're gathering various metrics under 3 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li><li><em>value+voting</em>: On top of above value workload, this one has DReps vote on and ratify governance actions - forcing additional computation for vote tallying and proposal enactment.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.1.1</code> shows an improvement of 4% (8% under Plutus workload) in Process CPU usage.</li><li>Allocation Rate improves by 8% (11% under Plutus workload), while Heap Size remains unchanged.</li><li>CPU 85% spans decrease by 18% (5% under Plutus workload).</li><li>Compared to value-only workload, ongoing voting leads to a slight increase of 5% in Process CPU usage.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Under Plutus workload, <code>10.1.1</code> exhibits a formidable speedup of 70ms in the forging loop - due to mempool snapshots being produced much more quickly.</li><li>Under value workload, there are no significant changes to forger metrics.</li><li>With voting added on top of the value workload, we can observe mempool snapshots and adoption time on the block producer rise by 10ms each.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration increases slightly by 16ms (or 5%) under value workload.</li><li>Under Plutus workload, there are no significant changes to peer-related metrics.</li><li>With the additional voting workload, peer adoption times rise by 12ms on average - confirming the observation for adoption time on the block producer.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li><code>10.1.1</code> exhibits a slight increase of 2% - 3% in cluster adoption times under value workload.</li><li>Under Plutus workload however, we observe significant improvement of 18% up to the 50th centile, and 9% - 13% in the 80th centile and above.</li><li>While the former is due to slightly increased Block Fetch duration, the latter is the consequence of much quicker mempool snapshots involving Plutus transactions.</li><li>Submitting the additional voting workload, we can observe a consistent 4% - 6% increase in cluster adoption times across all centiles.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>We do not detect any perfomance regression in <code>10.1.1</code> compared to <code>9.2.0</code>.</li><li>To the contrary - <code>10.1.1</code> is lighter on the Node process resource usage overall.</li><li>Improved forging and diffusion timings can be expected for blocks heavy on Plutus transactions.</li><li>Stressing the governance / voting capabalities of the Conway ledger lets us ascertain an (expected) performance cost of voting.</li><li>This cost has demonstrated to be reasonable, and to not contain lurking perfomance risks to the system.</li><li>It is expected to manifest only during periods of heavy vote tallying / proposal enactment, slightly affecting block adoption times.</li></ul><p>NB. The same amount of DReps are registered for each workload. However, only under <em>value+voting</em> do they become active by submitting votes. This requires an increased UTxO set size, so it uses
a baseline seperate from <em>value-only</em>, resulting in slightly different absolute values.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.1.value-only-0de4dbcc83d1c8caa8aa01fc857bf5ec.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.1.plutus-93a0ea846deddadcf6fa48be2df28f24.pdf">here</a>.  </p><p>Full report for <em>value+voting workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.1.voting-01ee31495edecaf8c36b79c94247919a.pdf">here</a>.  </p><p>NB. The release benchmarks for <code>10.1.1</code> were performed on tag <code>10.0.0-pre</code>. The minor version bump did not include changes relevant to performance; thus, measurements taken on <code>10.0.0-pre</code> remain valid.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 8.9.0]]></title>
        <id>2024-03-performance-8.9.0</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2024-03-performance-8.9.0"/>
        <updated>2024-03-13T09:34:49.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 3 different versions of <code>cardano-node</code>:</p><ul><li><code>8.7.2</code> - baseline for previous mainnet release</li><li><code>8.8.0</code> - an intermediate reference point</li><li><code>8.9.0</code> - the next mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><p>The observations stated refer to the direct comparison between the <code>8.7.2</code> and <code>8.9.0</code> versions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Overall CPU usage exhibits a small to moderate (5% - 8%) increase.</li><li>Memory usage is very slightly decreased by 1%.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>For full blocks, Mempool Snapshotting improves by 4% (or 3ms).</li><li>For small blocks, Self Adoption times improve by 8% (or 4ms).</li><li>All other forger metrics do not exhibit significant change.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>For full blocks, Block Fetch duration shows a notable improvement by 10ms (or 3%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><p>End-to-end propagation times on <code>8.9.0</code> exhibit a small improvement by 2% across all centiles for full blocks, whereas they remain largely unchanged for small blocks.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>The performance changes observed between <code>8.9.0</code> and <code>8.7.2</code> are only minor - with <code>8.9.0</code> slightly improving on <code>8.7.2</code>. Therefore, we'd expect <code>8.9.0</code> Mainnet performance to be akin to <code>8.7.2</code>.</li><li>We have demonstrated no performance regression has been introduced in <code>8.9.0</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.0.value-only-acff1460799d3a362997dc37816d39dd.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.0.plutus-2153f07c96f823ebea49f0bd3ed93e44.pdf">here</a>.</p><p>NB. Mainnet release <code>8.7.3</code> did not include any performance-related changes; measurements taken on <code>8.7.2</code> remain valid.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 8.9.1]]></title>
        <id>2024-03-performance-8.9.1</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2024-03-performance-8.9.1"/>
        <updated>2024-03-13T09:34:49.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>8.9.0</code> - baseline for previous mainnet release</li><li><code>8.9.1</code> - the next mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>We can observe an overall decrease in CPU usage (2% - 4%); only GC CPU usage under value workload increases by 3%.</li><li>Under value workload only, Allocation rate is very slightly decreased (1%) with no change to Heap Size.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Mempool Snapshot duration increases slightly by 2ms under value workload.</li><li>Self-Adoption time increases by 3ms.</li><li>All other forger metrics do not exhibit significant change.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under value workload only, Block Fetch duration and Fetched to Sending show a slight increase of 2ms each.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><p>End-to-end propagation times on <code>8.9.1</code> exhibit a small increase by 1% - 2% for full blocks, while remaining virtually unchanged for small blocks.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>The performance changes measured between <code>8.9.1</code> and <code>8.9.0</code> are very minor. Mainnnet performance of <code>8.9.1</code> is expected to be akin to <code>8.9.0</code>.</li><li>We have not observed any performance regression being introduced in <code>8.9.1</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.1.value-only-16f821f38b547d88c701881f741afffb.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.1.plutus-d9187a701bca584d89b3560f59a5e472.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 8.9.3]]></title>
        <id>2024-05-performance-8.9.3</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2024-05-performance-8.9.3"/>
        <updated>2024-03-13T09:34:49.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>8.9.1</code> - baseline from a previous mainnet release</li><li><code>8.9.3</code> - the current mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Under value workload, CPU usage increases slightly on <code>8.9.3</code>: 4% for Process, 3% for Mutator and 8% for GC.</li><li>Additionally, Allocation rate and minor GCs increase slightly by 3% each.</li><li>Under Plutus workload only, the GC live dataset increases by 10% or 318MB.</li><li>CPU 85% spans increase by 14% of slot duration under value workload, whereas they shorten by 5% of slot duration under Plutus workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>There are no significant changes to metrics related to block forging.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration improves by 7ms (or 2%) under value workload, and by 4ms (or 3%) under Plutus workload.</li><li>Under Plutus workload, Fetched to sending improves by 2ms (or 5%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload, cluster adoption times exhibit a minor improvement (1%) up to the 80th centile on <code>8.9.3</code>.</li><li>Under Plutus workload, we can observe a minor improvement overall (1% - 2%), whilst full adoption is unchanged.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>The performance changes measured between <code>8.9.3</code> and <code>8.9.1</code> are very minor, with <code>8.9.3</code> improving slightly over <code>8.9.1</code>.</li><li>Mainnnet performance of <code>8.9.3</code> is expected to be akin to <code>8.9.1</code>.</li><li>We have not observed any performance regression being introduced in <code>8.9.3</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.3.value-only-f08f2a51ca31fad16e2cb3e3ad28ee42.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.3.plutus-afc8f7bbf2f6f18fb56aede6169fff68.pdf">here</a>.</p><p>NB. The baseline for <code>8.9.1</code> had to be re-established due to changes in the underlying network infrastructure. This means, absolute values may differ from the previous measurements taken from that version.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 8.12.1]]></title>
        <id>2024-06-performance-8.12.1</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2024-06-performance-8.12.1"/>
        <updated>2024-03-13T09:34:49.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>8.9.3</code> - baseline from a previous mainnet release</li><li><code>8.12.1</code> - the current mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Under value workload, CPU usage is improved by 2% - 4%, and by 14% for GCs. Under Plutus workload, CPU usage improves only slightly by 1%.</li><li>Allocation Rate and Minor GCs improve by 5% and 6% - under Plutus workload, there's a slight improvement of 1%.</li><li>RAM usage is reduced by 3%; reduction under Plutus workload is even larger - namely 10%.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Mempool snapshotting improves by 5ms or 7% (3ms or 4% under Plutus workload).</li><li>Adoption time on the block producer improves by 4ms or 6% - under value workload only.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration increases slightly by 6ms or 2% (2ms under Plutus workload).</li><li>Adoption times on the peers improve slightly by 2ms or 3% (1ms under Plutus workload)</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload / full blocks there are no significant changes to cluster adoption times.</li><li>Under Plutus workload / small blocks we can observe a (near-jitter) improvement of 0% - 2% in cluster adoption times.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>The performance changes measured between <code>8.12.1</code> and <code>8.9.3</code> are most distinct in the resource usage footprint - with <code>8.12.1</code> improving over <code>8.9.3</code>.</li><li>On Mainnnet, <code>8.12.1</code> is expected to deliver equal or slightly better performance than <code>8.9.3</code> - as well as lowering the Node's resource usage somewhat in doing so.</li><li>We have not observed any performance regression being introduced in <code>8.12.1</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.12.1.value-only-d18eaee4dbf2ffb1c471c1b82e7ba499.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.12.1.plutus-bcf8b8e638f9a7d18710f76fc89e33da.pdf">here</a>.</p><p>NB. The release benchmarks for <code>8.12.1</code> were performed on tag <code>8.12.0-pre</code>. The patch version bump did not include changes relevant to performance; thus, measurements taken on <code>8.12.0-pre</code> remain valid.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 9.0.0]]></title>
        <id>2024-07-performance-9.0.0</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2024-07-performance-9.0.0"/>
        <updated>2024-03-13T09:34:49.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>8.12.1</code> - baseline from a previous mainnet release</li><li><code>9.0.0</code> - the current mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Under value workload Process and Mutator CPU usage are slightly higher on <code>9.0</code> - 7% - 8% (4% each under Plutus workload). GC CPU is increased by 11%, but decreases under Putus workload by 3%.</li><li>Only under value workload, Allocation Rate and Minor GCs increase by 5% and the live GC dataset grows by 3%. Heap size is constant.</li><li>CPU 85% spans are 8% shorter (3% under Plutus workload).</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Mempool Snapshotting and Self Adoption time on the block producer increase very slightly under value workload - 2ms (or 3%) each.</li><li>Under Plutus workload, however, a decrease in Self Adoption time by 2ms (or 4%) is the only significant change in the forging loop.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration is 21ms faster (6%) - 7ms or 5% under Plutus workload.</li><li>Fetched to Sending increases slightly by 3ms (7%) - only under value workload.</li><li>Adoption times on the peers increase slightly by 4ms (5%) - under Plutus workload, however, they are 3ms (6%) faster.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload / full blocks on <code>9.0</code>, we can observe a 4% - 5% improvement of cluster adoption times in the 80th centile and above.</li><li>Under Plutus workload / small blocks, the corresponding improvement is 5% - 6%.</li><li>The main contributing factor is the improvement in Block Fetch duration.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>Network performance clearly improves by ~%5 for 80% to full cluster adoption - independent of workload.</li><li>RAM usage is unchanged on <code>9.0</code>. The slight rise in CPU usage is expected, given improved network performance, and does not pose cause for concern.</li><li>We have not observed any performance regression being introduced in <code>9.0.0.</code>.</li></ul><p>NB. These benchmarks were performed in the Conway ledger era. As such, they do not cover the one-time performance cost of transitioning from Babbage and enabling the new features of the Conway ledger.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-9.0.0.value-only-8ef8064d6bb04de733985e12e9a9e5ea.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-9.0.0.plutus-8385e89a67774584329c93e59dcfe545.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 9.2.0]]></title>
        <id>2024-09-performance-9.2.0</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2024-09-performance-9.2.0"/>
        <updated>2024-03-13T09:34:49.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>9.1.1</code> - baseline from a previous mainnet release</li><li><code>9.2.0</code> - the current mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Under value workload, <code>9.2.0</code> shows an increase by 7% in Process CPU usage.</li><li>Additionally, Allocation Rate and Minor GCs increase by 6% each, while Heap Size remains unchanged.</li><li>Furthermore, CPU 85% spans increase by 10%.</li><li>Under Plutus workload however, there's just one significant observation: a larger portion of the heap is considered live (6% or ~190MB) with the overall Heap Size remaining constant.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>For the forger metrics, we can observe minor (1ms - 2ms) improvements in Ledger Ticking, Mempool Snapshotting and Self Adoption under value workload.</li><li>Under Plutus workload, there are minor (1ms - 2ms) increases in Ledger Ticking and Mempool Snapshotting.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration has improved by 11ms (or 3%) under value workload.</li><li>Under Plutus workload, peer Adoption times are slightly increased by 2ms (3%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload / full blocks, <code>9.2.0</code> exhibits a slight improvement of 1% - 3% in cluster adoption times.</li><li>Under Plutus workload / small blocks, there's a very minor increase by 1%.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>We can not detect any perfomance regression in <code>9.2.0</code> compared to <code>9.1.1</code>.</li><li>Under heavy value workload, <code>9.2.0</code> seems to perform work somewhat more eagerly. This would correlate with the slightly increased CPU usage, but also with the improvements in the forging and peer related metrics.</li><li>The clearly increased efficiency of Block Fetch under heavy workload is the main contributing factor to the slight overall network performance improvement.</li></ul><p>NB. These benchmarks were performed using an adjusted, post-Chang hardfork performance baseline to account for added features in the Conway ledger era. Thus, absolute measurements might differ now from those taken using the previous baseline.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-9.2.0.value-only-7c4ead8e5f6e6f965caeb9e7b34cf54d.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-9.2.0.plutus-5b1787d5b946bea7b9560731149bb975.pdf">here</a>.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking -- Node 8.7.2]]></title>
        <id>2023-12-performance-8.7.2</id>
        <link href="https://updates.cardano.intersectmbo.org/reports/2023-12-performance-8.7.2"/>
        <updated>2023-12-08T15:38:20.000Z</updated>
        <summary type="html"><![CDATA[Setup]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 3 different versions of <code>cardano-node</code>:</p><ul><li><code>8.1.2</code> - the last mainnet release</li><li><code>8.7.0-pre</code> - as an intermediate reference point</li><li><code>8.7.2</code> - the next mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><p>The observations stated refer to the direct comparison between the <code>8.1.2</code> and <code>8.7.2</code> versions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Plutus workload, having a lower overall absolute CPU load, exhibits an average increase of 27% in Process CPU usage. Value workload, having a higher overall absolute CPU load, exhibits a near-jitter increase of 1%.</li><li>Allocation rates increase by ~8.9MB/s (value workload) and ~12.6MB/s (Plutus workload).</li><li>Heap sizes increase by 47% - 54%.</li><li>CPU 85% span duration shrinks by ~9.7 slots under value workload, and ~5.8 slots under Pluts workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Block Context Acquisition in the forging loop increases by ~10ms.</li><li>Mempool snapshotting shows an increase by 16ms under value workload; under Plutus workload, it increases by 3ms.</li><li>Ledger ticking improves slightly by 1-2ms.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachements) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block fetch time increases for full blocks by 9%. For small blocks, it improves by 7%.</li><li>Time to resend a block after fetching increases by 8% for full blocks, whereas it improves by 2% for small blocks.</li><li>Block adoption by a peer takes 12% more time for a full block, but happens faster by 4% for a small block.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.</p><p>The metric exhibits an increase by ~10% across all centiles for full blocks, whereas it improves by 5-6% for small blocks in the higher (80th and above) centiles.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>This is the first time we're publishing, to a wider audience, such benchmarking results. We are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/8.7.1_8.1.2_8.7.0-pre_8.7.1-pre.value-only-fdb343a09234cd47578ec1ec47c4610e.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/8.7.1_8.1.2_8.7.0-pre_8.7.1-pre.plutus-f38724e73290c55323955558c17f2590.pdf">here</a>.</p><p>The relese benchmarks for <code>8.7.2</code> were performed on tag <code>8.7.1-pre</code>, which features identical <code>cardano-node</code> components.</p>]]></content>
        <author>
            <name>Michael Karg</name>
            <uri>https://github.com/mgmeier</uri>
        </author>
        <category label="benchmarking-reports" term="benchmarking-reports"/>
    </entry>
</feed>