<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Cardano Development Updates Blog</title>
        <link>https://updates.cardano.intersectmbo.org/reports</link>
        <description>Cardano Development Updates Blog</description>
        <lastBuildDate>Wed, 02 Jul 2025 09:50:10 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Benchmarking -- Node 10.5.0]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2025-07-performance-10.5.0</link>
            <guid>2025-07-performance-10.5.0</guid>
            <pubDate>Wed, 02 Jul 2025 09:50:10 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.4.1</code> - baseline from the previous Node release</li><li><code>10.5.0</code> - the current (pre-)release tag</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="preliminaries">Preliminaries<a class="hash-link" href="#preliminaries" title="Direct link to heading">​</a></h2><ol><li>The feature in <code>10.5</code> with major performance impact is periodic <a href="https://github.com/IntersectMBO/cardano-node/pull/6180" target="_blank" rel="noopener noreferrer">ledger metrics</a>. This is exclusive to the <a href="https://developers.cardano.org/docs/get-started/cardano-node/new-tracing-system/new-tracing-system" target="_blank" rel="noopener noreferrer">new tracing system</a>.</li><li><code>10.5</code> flips the default config for <code>PeerSharing</code> to <code>true</code>; however, the recommendation is to explicitly set it to <code>false</code> on block producers. If not for privacy issues alone, we also found disadvantageous performance impact on block production when enabled. Hence, our benchmarks do not factor in that overhead.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.5.0</code> shows a clear reduction in CPU usage - by ~30% regardless of workload type.</li><li>Furthermore, Allocation rate and GC impact are clearly reduced - by 27%-29% and 24%-25% respectively.</li><li>Heap size <em>increases</em> very slightly under saturation (by 1%) and <em>decreases</em> very slightly (by 1%) under Plutus workload.</li><li>CPU 85% spans are slightly <em>shorter</em> (~0.2 slots) under saturation, and slightly <em>longer</em> (~0.26 slots) under Plutus workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Block Context Acquisition time (prior to leadership check) is greatly reduced - from ~24ms to under 1ms.</li><li>Under saturation only, Ledger Ticking and Mempool Snapshotting exhibit very slight upticks (by 3ms and 2ms respectively).</li><li>Under Plutus workload only, Self Adoption on the forger exhibits a very slight uptick (by 3ms).</li><li>In summary, a block producer is able to announce a new header 20ms or 21% earlier into the slot (22ms or 43% under Plutus workload).</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under saturation workload only, Block Fetch duration increases by 14ms (or 4%).</li><li>Under saturation, block adoption is slightly <em>faster</em> (by 3ms), while under Plutus workload it's slightly <em>slower</em> (by 2ms).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under saturation workload, cluster adoption times on <code>10.5.0</code> are identical to those on <code>10.4.1</code>.</li><li>Under Plutus workload, they show a moderate 3% - 5% <em>improvement</em>, with 7% in the 50th percentile.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>We could not detect any regressions or performance risks to the network on <code>10.5.0</code>.</li><li>CPU usage is clearly reduced.</li><li>The forging loop executes faster, new header announcements happen earlier.</li><li>Diffusion / adoption metrics exhibit a small overall improvement and indicate <code>10.5.0</code> will deliver network performance at least comparable to <code>10.4.1</code>.</li><li>All improvements listed above hinge on the <a href="https://github.com/IntersectMBO/cardano-node/pull/6180" target="_blank" rel="noopener noreferrer">ledger metrics</a> feature and will materialize only when using the <a href="https://developers.cardano.org/docs/get-started/cardano-node/new-tracing-system/new-tracing-system" target="_blank" rel="noopener noreferrer">new tracing system</a>. Using the legacy system, <code>10.5.0</code> performance is expected to be almost identical to <code>10.4.1</code>.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full comparison for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.5.0.value-only-c886a172ada813c722a3e366f9a8ff46.pdf">here</a>.</p><p>Full comparison for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.5.0.plutus-8be2a58dd7bc67d41af6254b47ce4a58.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.5.0</code> extend to a potential <code>10.5.1</code> tag, as that won't include any changes with a performance impact; thus, measurements performed on <code>10.5.0</code> remain valid.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 10.4.1]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2025-05-performance-10.4.1</link>
            <guid>2025-05-performance-10.4.1</guid>
            <pubDate>Mon, 05 May 2025 15:29:39 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.3.1</code> - baseline from the previous Node release</li><li><code>10.4.1</code> - the current release</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><p><code>10.4.1</code> features the UTxO-HD in-memory backing store <code>V2InMemory</code> of <code>LedgerDB</code>, which replaces the in-memory representation of UTxO entries in <code>10.3</code> and prior.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>On <code>10.4.0</code> under value workload, Heap size increases slightly by 2%, and 5% under Plutus workload. This corresponds to using ~170MiB-390MiB additional RAM.</li><li>Allocation rate and GC impact are virtually unchanged.</li><li>Process CPU usage improves slightly by 2% regardless of workload type.</li><li>CPU 85% spans are slightly (~0.37 slots) <em>longer</em> under value workload, and slightly <em>shorter</em> (~0.33) under Plutus workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>We can observe a clear improvement in Mempool snapshotting by 9ms or 16% (2ms or 8% under Plutus workload).</li><li>Self-Adoption time improves by 4ms or 5% (and remains virtually unchanged under Plutus workload).</li><li>Hence a block producer is able to announce a new header 10ms or 9% earlier into the slot (1ms or 2% under Plutus workload).</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under value workload, Fetch duration and Fetched to Sending improve slightly by 3ms (1%) and 2ms (4%).</li><li>Under Plutus workload, Fetched to Sending has a slightly longer delay - 2ms (or 5%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload, cluster adoption times exhibit a small 1% - 3% <em>improvement</em> across all percentiles.</li><li>Under Plutus workload, they show a small 1% - 2% <em>increase</em> across all percentiles (except the 80th).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>We could not detect any regressions or performance risks to the network on <code>10.4.1</code>.</li><li>There is a small and reasonable price to pay in RAM usage for adding the <code>LedgerDB</code> abstraction and thus enable exchangeable backing store implementations.</li><li>On the other hand, CPU usage is reduced slightly by use of the in-memory backing store.</li><li><code>10.4.1</code> is beneficial in all cases for block production metrics; specifically, block producers will be able to announce new headers earlier into the slot.</li><li>Network diffusion and adoption metrics vary only slightly and indicate <code>10.4.1</code> will deliver network performance comparable to <code>10.3.1</code>.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full comparison for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.4.1.value-only-d204944948caf9a791ebe994a60cc9dd.pdf">here</a>.</p><p>Full comparison for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.4.1.plutus-1cf67914451f54090b7441b69fb5579a.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.4.1</code> were performed on tag <code>10.4.0</code>. The patch version bump did not include changes relevant to performance; thus, measurements performed on <code>10.4.0</code> remain valid. The same holds for <code>10.3.1</code> and <code>10.3.0</code>.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 10.3.1]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2025-04-performance-10.3.1</link>
            <guid>2025-04-performance-10.3.1</guid>
            <pubDate>Tue, 22 Apr 2025 13:54:50 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 3 different versions of <code>cardano-node</code>:</p><ul><li><code>10.2</code> - baseline from the previous release (bulit with GHC8.10.7)</li><li><code>10.3.0-ghc8107</code> - the current release built with GHC8.10.7</li><li><code>10.3.0-ghc965</code> - the current release built with GHC9.6.5</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.  </p><p><code>10.3.1</code> supports two compiler versions, which will be taken into account when comparing performance of different builds of that release.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.3.1</code> exhibits a clear reduction in Process CPU usage, more prominent under <em>value workload</em>:<ul><li>value workload: 10% with GHC8, and 24% with GHC9.</li><li>Plutus workload: 4% GHC8, and 6% with GHC9.</li></ul></li><li>There also is a reduction in RAM usage, more prominent under <em>Plutus workload</em>:<ul><li>value workload: 1% or ~54MiB with GHC8, and 6% or ~574MiB with GHC9.</li><li>Plutus workload: 14% or ~1.2GiB with GHC9 only.</li></ul></li><li>Minor GCs and Allocation rate both drop on <code>10.3.1</code>, more significantly under <em>value workload</em>:<ul><li>value workload: 11% each with GHC8, and 24% each with GHC9.</li><li>Plutus workload: 3% and 1% with GHC8; 5% and 4% with GHC9. </li></ul></li><li>Under value workload, CPU 85% spans <em>increase</em> by 45% with GHC8, but only by 14% with GHC9.</li><li>Under Plutus workload, those spans <em>decrease</em> by 5% with GHC8; even by 19% with GHC9.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Under value workload, several block production metrics improve clearly on <code>10.3.1</code>, most prominently Mempool Snapshotting.</li><li>With GHC8, the improvement is 23%, with further significant improvements in Adoption time (11%) and Ledger ticking (10%).</li><li>With GHC9, the improvement is 27%, with further significant improvements in Adoption time (10%) and Ledger ticking (17%).</li><li>Under value workload, this enables a block producer to announce a header earlier into the slot, namely by 23ms (GHC8) and by 28ms (GHC9).</li><li>Under Plutus workload, Adoption time <em>increases</em> by 3ms (6%) with GHC8, but <em>decreases</em> by 8ms (15%) with GHC9.</li><li>Furthermore, there are no significant changes to the header announcement timing.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under value-only workload only, we observe an increase in Block Fetch duration: 7ms (2%) with GHC8, and 23ms (7%) with GHC9.</li><li>Block adoption times on the peers improve clearly: 11ms (12%) with GHC8, and 12ms (14%) with GHC9.</li><li>Under Plutus workload, however, similarly to the block producer, adoption times <em>increase</em> by 3ms (6%) with GHC8, but <em>decrease</em> by 7ms (13%) with GHC9.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload, cluster adoption times on <code>10.3.1</code> are largely unchanged.</li><li>With GHC8, there are 5% and 3% improvements in the 50th and 100th centiles; with GHC9, there's a small 3% improvement in the 50th centile.</li><li>Under Plutus workload, with GHC8, there's a moderate 6% <em>increase</em> in cluster adoption times in the 100th centile.</li><li>With GHC9, however, there's a small 2% <em>improvement</em> in all but the 100th centile.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>For <code>10.3.1</code> we could not detect any performance risks or regressions.</li><li>Improving resource usage was a stated goal for the <code>10.3</code> release; this could be confirmed via measurements for CPU and RAM usage as well as CPU spikes.</li><li><code>10.3.1</code> achieves network performance comparable to <code>10.2.1</code> using clearly less system resources - for both compiler versions.</li><li>Several key metrics improve on <code>10.3.1</code>: Block producers announce a new header sooner into the slot; we observe lower adoption times (GHC9 only).</li><li>The GHC9.6.5 build has demonstrable performance advantages over the GHC8.10.7 build; especially the Plutus interpreter seems to gain considerably from using GHC9. For those reasons we now recommend <em>GHC9.6.x</em> for production builds.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.3.1.value-only-cfaa4837c218e103c2b151a4bd1a030a.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.3.1.plutus-45a3a1b97e744ee88194002a805d665a.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.3.1</code> were performed on tag <code>10.3.0</code>. The patch version bump did not include changes relevant to performance; thus, measurements performed on <code>10.3.0</code> remain valid.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Memory Budget Scaling -- 10.2]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2025-03-execbudget-memory-10.2</link>
            <guid>2025-03-execbudget-memory-10.2</guid>
            <pubDate>Tue, 01 Apr 2025 09:50:10 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>This report compares benchmarking runs for 3 different settings of the Plutus memory execution budget:</p><ul><li><code>loop-memx1</code> - current mainnet memory execution budget</li><li><code>loop-memx1.5</code> - 1.5 x current mainnet memory execution budget</li><li><code>loop-memx2</code> - 2 x current mainnet memory execution budget</li></ul><p>For this comparison, we gather various metrics under the <em>Plutus</em> workload used in release benchmarks: Each block produced during the benchmark contains
4 identical script transactions calibrated to fully exhaust the memory execution budget. Thus, script execution is constrained by the memory budget limit
every case. The workload produces small blocks (&lt; 3kB) exclusively.  </p><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. Node
version 10.2 was used, built with GHC8.10.7.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget impacts Allocation Rate and Minor GCs. 1.5 x the budget results in rises of 5% and 6% respecetively; for doubling the budget the corresponding rises are 10% and 11%.</li><li>Those increases seem to correlate linearly with raising mem budget.</li><li>The effect on CPU usage is almost negligible: a 1% (or 3%, for doubling the budget) increase of Process CPU.</li><li>The Node process RAM footprint is unaffected.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget has significant impact on block adoption time only.</li><li>Scaling by factor 1.5 leads to a 14ms (or 25%) increase, whereas factor 2 leads to 28ms (49%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Same as on the block producer, scaling the memory budget has significant impact on block adoption times only.</li><li>Scaling by factor 1.5 leads to a 15ms (or 26%) increase, whereas factor 2 leads to 28ms (48%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>1.5 x the memory budget results in a slight increase of 19ms - 22ms in cluster adoption times (4% - 5%).</li><li>2 x the memory budget results in a moderate 27ms - 34ms increase (5% - 7%, with 9% in the 50th centile).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><p>These measurements outline the headroom for raising the memory budget, along with the expected performance impact:</p><ol><li>Block adoption time is the only metric that's affected significantly, increasing both on the forger and the peers by the same extent.</li><li>These increases seem to correspond linearly with the raising the memory budget. This gives excellent predictability of performance impact.</li><li>Expectedly, more allocations happen; we can observe the same linear correspondence here as well.</li><li>It has to be pointed out that block diffusion is only slightly affected by changing the execution budget: Due to pipelining, announcing and (re-)sending a block precedes adoption in most cases.</li><li>As such, regarding absolute cluster adoption times, measurements taken with either budget adjustment do not exhibit performance risks to the network. They do illustrate, however, the performance cost of those budget adjustments.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachment">Attachment<a class="hash-link" href="#attachment" title="Direct link to heading">​</a></h2><p>Full report PDF downloadable <a target="_blank" href="/assets/files/execbudget-10.2-mem_scaling-ae344917bf4358c013821b4a7449f283.pdf">here</a>.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Memory Budget Scaling -- 10.3]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2025-05-execbudget-memory-10.3</link>
            <guid>2025-05-execbudget-memory-10.3</guid>
            <pubDate>Tue, 01 Apr 2025 09:50:10 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>This report compares benchmarking runs for 3 different settings of the Plutus memory execution budget:</p><ul><li><code>10.3-ghc965</code> - current mainnet memory execution budget</li><li><code>loop-memx1.5</code> - 1.5 x current mainnet memory execution budget</li><li><code>loop-memx2</code> - 2 x current mainnet memory execution budget</li></ul><p>For this comparison, we gather various metrics under the <em>Plutus</em> workload used in release benchmarks: Each block produced during the benchmark contains
4 identical script transactions calibrated to fully exhaust the memory execution budget. Thus, script execution is constrained by the memory budget limit
every case. The workload produces small blocks (&lt; 3kB) exclusively. </p><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. Node
version 10.3 was used, built with GHC9.6.5. This is a re-run of the scaling benchmarks performed on Node version 10.2 / GHC8.10 to document impact of performance improvements. Those results
were published here on <a href="https://updates.cardano.intersectmbo.org/reports/2025-03-execbudget-memory-10.2" target="_blank" rel="noopener noreferrer">Cardano Updates</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget impacts Allocation Rate and Minor GCs. 1.5 x the budget results in rises of 5% each; for doubling the budget the corresponding rises are 8% and 9%.</li><li>Those increases seem to correlate linearly with raising mem budget.</li><li>The effects on CPU usage and RAM footprint are negligible for both scaling factors.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Scaling the memory budget has significant impact on block adoption time only.</li><li>Scaling by factor 1.5 leads to a 10ms (or 24%) increase, whereas factor 2 leads to 21ms (50%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Same as on the block producer, scaling the memory budget has significant impact on block adoption times only.</li><li>Scaling by factor 1.5 leads to a 11ms (or 24%) increase, whereas factor 2 leads to 19ms (42%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>1.5 x the memory budget results in a slight increase of  9ms - 31ms in cluster adoption times (3% - 6%).</li><li>2 x the memory budget results in a moderate 17ms - 35ms increase (5% - 7%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><p>These measurements outline the headroom for raising the memory budget, along with the expected performance impact:</p><ol><li>Block adoption time is the only metric that's affected significantly, increasing both on the forger and the peers by the same extent.</li><li>These increases seem to correspond linearly with the raising the memory budget. This gives excellent predictability of performance impact.</li><li>Expectedly, more allocations happen; we can observe the same linear correspondence here as well.</li><li>It has to be pointed out that block diffusion is only slightly affected by changing the execution budget: Due to pipelining, announcing and (re-)sending a block precedes adoption in most cases.</li><li>As such, regarding absolute cluster adoption times, measurements taken with either budget adjustment do not exhibit performance risks to the network. They do illustrate, however, the performance cost of those budget adjustments.</li></ol><p>These scaling benchmarks are complementary to those performed on Node 10.2; in comparison with those, we can additionally conclude:</p><ol><li>The conclusions from measurements for each scaling run set are identical.</li><li>While the relative increases in adoption time for both Node builds are quite similar, the absolute increases are 22% - 32% <em>smaller</em> (i.e., adoption happens more efficiently) for 10.3 / GHC9.6.</li><li>The same rationale applies to end-to-end propagation metrics: Absolute values document faster cluster adoption for 10.3 / GHC9.6. </li><li>Incidentally, the absolute values for scaling factor 1 on 10.2 are close to those for scaling factor 2 on 10.3 except for the tail end (i.e. 95th percentile and above).</li><li>This reflects the performance improvements that were a stated goal for the 10.3 release - and suggests the performance cost of memory budget increases has become slightly <em>smaller</em> in absolute terms.</li></ol><p>As adoption times are not only impacted by Plutus execution alone, we still advocate for a conservative and/or multi-stage raise; future backpedaling on budget limits could cause issues for scripts already deployed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachment">Attachment<a class="hash-link" href="#attachment" title="Direct link to heading">​</a></h2><p>Full report PDF downloadable <a target="_blank" href="/assets/files/execbudget-10.3-mem_scaling-af25748ab9933bb24a20c245b885dc7e.pdf">here</a>.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- UTxO-HD on 10.2]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2025-02-performance-utxohd-10.2</link>
            <guid>2025-02-performance-utxohd-10.2</guid>
            <pubDate>Fri, 21 Feb 2025 17:25:57 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>This report compares benchmarking runs for 2 different flavours of <code>cardano-node</code>:</p><ul><li><code>10.2-regular</code> - regular Node performance baseline from the <code>10.2.x</code> release benchmarks.</li><li><code>10.2-utxohd</code> - the UTxO-HD build of the Node based on that same version.</li></ul><p>For this benchmark, we're gathering various metrics under the <em>value-only</em> workload used in release benchmarks: Each transaction consumes 2 inputs and creates 2 outputs,
changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively. Moreover, it's the workload that produces most stress on the UTxO set. Thus, it's the most meaningful
workload when it comes to benchmarking UTxO-HD.  </p><p>We target the <em>in-memory backing store</em> of UTxO-HD - LedgerDB V2 in this case. The on-disk backend is not used. </p><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>With UTxO-HD's in-memory backend, the memory footprint increases slightly by 3%.</li><li>Process CPU usage is moderately reduced by 9% with UTxO-HD.</li><li>Additionally, CPU 85% spans decrease in duration by 24% (~1.1 slots).</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Block context acquisition improves by 3ms (or 11%), while Ledger ticking takes 3ms (or 10%) longer.</li><li>Creating a mempool snapshot is significantly faster - by 16ms (or 21%).</li><li>As a result, a UTxO-HD block producing node is able to announce a new header 17ms (or 12%) earlier into a slot.</li><li>Additionally, adoption time on the forger is slightly improved - by 4ms (or 5%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block fetch duration increases moderately by 13ms or 4%.</li><li>Adoption times on the peers improve very slightly - by 2ms or 2%.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>There is no significant difference in cluster adoption times between regular and UTxO-HD node.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><p>Regarding the UTxO-HD build using the in-memory LedgerDB V2 backend, we can conclude that:</p><ol><li>it is lighter on CPU usage compared to the regular node, albeit requiring just slightly more RAM.</li><li>it poses no performance risk to block producers; on the contrary, the changes in forging loop metrics seem favourable compared to the regular node.  </li><li>network performance would be expeceted to be on par with the regular node.</li><li>even under stress, there is no measurable performance regression compared to the regular node.</li><li>as a consequence of the above, performance-wise, it's a viable replacement for the regular in-memory solution.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachment">Attachment<a class="hash-link" href="#attachment" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/utxohd-10.2.value-only-6fc915d9fd584640513356bc53e858ea.pdf">here</a>.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 10.2.1]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2025-02-performance-10.2.1</link>
            <guid>2025-02-performance-10.2.1</guid>
            <pubDate>Fri, 21 Feb 2025 13:39:30 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.1.4</code> - baseline from a previous mainnet release</li><li><code>10.2.1</code> - the current release</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>CPU usage increases moderately by 12% under value, and very slightly by 2% under Plutus workload.</li><li>CPU 85% spans increase by 14% (~0.6 slots) under value workload, but decrease by 6% (~0.8 slots) under Plutus workload.</li><li>Only under value workload, we observe a slight increase in Allocation rate and Minor GCs of 9% and 8%</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Adoption time on the forger improves by 3ms (or 4%) - and 5ms (or 9%) under Plutus workload.</li><li>Block context acquisition takes 3ms (or 12%) longer under value workload.</li><li>Under Plutus workload only, ledger ticking improves by 3ms (or 12%).</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block fetch duration improves clearly by 16ms (or 4%) under value-only workload.</li><li>Under Plutus workload, we can measure an improvement by 4ms (or 7%) for adoption times on the peers.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><p>As a result of the above, on <code>10.2.1</code> exhibits:</p><ol><li>a slight 3% improvement in cluster adoption times in the 80th centile and above under value workload.</li><li>a near-jitter 1% - 3% improvement in cluster adoption times under Plutus workload.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>We could not detect any significant regressions, or performance risks, on <code>10.2.1</code>.</li><li><code>10.2.1</code> comes with slightly increased CPU usage, and no changes to RAM footprint.</li><li>Diffusion metrics very slightly improve - mainly due to block fetch being more efficient for full blocks, and adoption for blocks exclusively containing Plutus transactions.</li><li>This points to network performance of <code>10.2.1</code> being on par with or very slightly better than <code>10.1.4</code>.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.2.1.value-only-5cf6817ff642bf5a9fb8e80e4acbbc1c.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.2.1.plutus-253a36c1df440f4c9b5a3041ed8640e3.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.2.1</code> were performed on tag <code>10.2.0</code>. The patch version bump did not include changes relevant to performance; thus, measurements and observations performed on <code>10.2.0</code> remain valid.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 10.1.4]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2025-01-performance-10.1.4</link>
            <guid>2025-01-performance-10.1.4</guid>
            <pubDate>Fri, 10 Jan 2025 12:16:13 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>10.1.1</code> - baseline from a previous mainnet release</li><li><code>10.1.4</code> - the current mainnet release</li></ul><p>For this benchmark, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>CPU 85% spans slightly increase by 6% or ~0.2 slots (26% or ~2.9 slots under Plutus workload).</li><li>We can observe a tiny increase in memory usage by 1-2% (132-160 MiB).</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Under value workload, Ledger Ticking and Self Adoption exhibit a very slight increase (2ms each).</li><li>Block Context Acquisition has improved by 2ms.</li><li>Under Plutus workload, there are no significant changes to forger metrics.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>There's a minor increase of 1% (3ms) in Block Fetch duration under value workload only.</li><li>Under Plutus workload, we can measure a small improvement by 2% for adoption times on the peers.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><p>As a result of the above, on <code>10.1.4</code> we can observe:</p><ol><li>a tiny increase in cluster adoption times of 1%-2% in the 80th centile and above under value workload.</li><li>an improvement in cluster adoption times of 3%-4% in the tail end (95th centile and above) under Plutus workload.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ol><li>For <code>10.1.4</code>, we could not detect any regressions or performance risks.</li><li>All increases or decreases in forger and peer metrics are 3ms and less. This indicates network performance of <code>10.1.4</code> will very closely match that of <code>10.1.1</code> and subsequent patch releases.</li><li>There's no significant change in the resource usage pattern. The increased CPU 85% spans tend to barely manifest when the system is under heavy load (value workload); as such, they pose no cause for concern. </li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.4.value-only-7e81a4debde7bea0f719279ffb8ebd51.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.4.plutus-6272430f9ef6c8d53cfeaacced5bf04c.pdf">here</a>.  </p><p>NB. The benchmarks for <code>10.1.1</code> were performed on tag <code>10.0.0-pre</code>. The minor version bump did not include changes relevant to performance; thus, measurements taken on <code>10.0.0-pre</code> remain a valid baseline.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 10.1.1]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2024-10-performance-10.1.1</link>
            <guid>2024-10-performance-10.1.1</guid>
            <pubDate>Thu, 31 Oct 2024 11:03:27 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>9.2.0</code> - baseline from a previous mainnet release</li><li><code>10.1.1</code> - the current mainnet release</li></ul><p>For this benchmark, we're gathering various metrics under 3 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li><li><em>value+voting</em>: On top of above value workload, this one has DReps vote on and ratify governance actions - forcing additional computation for vote tallying and proposal enactment.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li><code>10.1.1</code> shows an improvement of 4% (8% under Plutus workload) in Process CPU usage.</li><li>Allocation Rate improves by 8% (11% under Plutus workload), while Heap Size remains unchanged.</li><li>CPU 85% spans decrease by 18% (5% under Plutus workload).</li><li>Compared to value-only workload, ongoing voting leads to a slight increase of 5% in Process CPU usage.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Under Plutus workload, <code>10.1.1</code> exhibits a formidable speedup of 70ms in the forging loop - due to mempool snapshots being produced much more quickly.</li><li>Under value workload, there are no significant changes to forger metrics.</li><li>With voting added on top of the value workload, we can observe mempool snapshots and adoption time on the block producer rise by 10ms each.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration increases slightly by 16ms (or 5%) under value workload.</li><li>Under Plutus workload, there are no significant changes to peer-related metrics.</li><li>With the additional voting workload, peer adoption times rise by 12ms on average - confirming the observation for adoption time on the block producer.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li><code>10.1.1</code> exhibits a slight increase of 2% - 3% in cluster adoption times under value workload.</li><li>Under Plutus workload however, we observe significant improvement of 18% up to the 50th centile, and 9% - 13% in the 80th centile and above.</li><li>While the former is due to slightly increased Block Fetch duration, the latter is the consequence of much quicker mempool snapshots involving Plutus transactions.</li><li>Submitting the additional voting workload, we can observe a consistent 4% - 6% increase in cluster adoption times across all centiles.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>We do not detect any perfomance regression in <code>10.1.1</code> compared to <code>9.2.0</code>.</li><li>To the contrary - <code>10.1.1</code> is lighter on the Node process resource usage overall.</li><li>Improved forging and diffusion timings can be expected for blocks heavy on Plutus transactions.</li><li>Stressing the governance / voting capabalities of the Conway ledger lets us ascertain an (expected) performance cost of voting.</li><li>This cost has demonstrated to be reasonable, and to not contain lurking perfomance risks to the system.</li><li>It is expected to manifest only during periods of heavy vote tallying / proposal enactment, slightly affecting block adoption times.</li></ul><p>NB. The same amount of DReps are registered for each workload. However, only under <em>value+voting</em> do they become active by submitting votes. This requires an increased UTxO set size, so it uses
a baseline seperate from <em>value-only</em>, resulting in slightly different absolute values.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.1.value-only-0de4dbcc83d1c8caa8aa01fc857bf5ec.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.1.plutus-93a0ea846deddadcf6fa48be2df28f24.pdf">here</a>.  </p><p>Full report for <em>value+voting workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-10.1.1.voting-01ee31495edecaf8c36b79c94247919a.pdf">here</a>.  </p><p>NB. The release benchmarks for <code>10.1.1</code> were performed on tag <code>10.0.0-pre</code>. The minor version bump did not include changes relevant to performance; thus, measurements taken on <code>10.0.0-pre</code> remain valid.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 8.9.0]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2024-03-performance-8.9.0</link>
            <guid>2024-03-performance-8.9.0</guid>
            <pubDate>Wed, 13 Mar 2024 09:34:49 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 3 different versions of <code>cardano-node</code>:</p><ul><li><code>8.7.2</code> - baseline for previous mainnet release</li><li><code>8.8.0</code> - an intermediate reference point</li><li><code>8.9.0</code> - the next mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><p>The observations stated refer to the direct comparison between the <code>8.7.2</code> and <code>8.9.0</code> versions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Overall CPU usage exhibits a small to moderate (5% - 8%) increase.</li><li>Memory usage is very slightly decreased by 1%.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>For full blocks, Mempool Snapshotting improves by 4% (or 3ms).</li><li>For small blocks, Self Adoption times improve by 8% (or 4ms).</li><li>All other forger metrics do not exhibit significant change.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>For full blocks, Block Fetch duration shows a notable improvement by 10ms (or 3%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><p>End-to-end propagation times on <code>8.9.0</code> exhibit a small improvement by 2% across all centiles for full blocks, whereas they remain largely unchanged for small blocks.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>The performance changes observed between <code>8.9.0</code> and <code>8.7.2</code> are only minor - with <code>8.9.0</code> slightly improving on <code>8.7.2</code>. Therefore, we'd expect <code>8.9.0</code> Mainnet performance to be akin to <code>8.7.2</code>.</li><li>We have demonstrated no performance regression has been introduced in <code>8.9.0</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.0.value-only-acff1460799d3a362997dc37816d39dd.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.0.plutus-2153f07c96f823ebea49f0bd3ed93e44.pdf">here</a>.</p><p>NB. Mainnet release <code>8.7.3</code> did not include any performance-related changes; measurements taken on <code>8.7.2</code> remain valid.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 8.9.1]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2024-03-performance-8.9.1</link>
            <guid>2024-03-performance-8.9.1</guid>
            <pubDate>Wed, 13 Mar 2024 09:34:49 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>8.9.0</code> - baseline for previous mainnet release</li><li><code>8.9.1</code> - the next mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>We can observe an overall decrease in CPU usage (2% - 4%); only GC CPU usage under value workload increases by 3%.</li><li>Under value workload only, Allocation rate is very slightly decreased (1%) with no change to Heap Size.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Mempool Snapshot duration increases slightly by 2ms under value workload.</li><li>Self-Adoption time increases by 3ms.</li><li>All other forger metrics do not exhibit significant change.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Under value workload only, Block Fetch duration and Fetched to Sending show a slight increase of 2ms each.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><p>End-to-end propagation times on <code>8.9.1</code> exhibit a small increase by 1% - 2% for full blocks, while remaining virtually unchanged for small blocks.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>The performance changes measured between <code>8.9.1</code> and <code>8.9.0</code> are very minor. Mainnnet performance of <code>8.9.1</code> is expected to be akin to <code>8.9.0</code>.</li><li>We have not observed any performance regression being introduced in <code>8.9.1</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.1.value-only-16f821f38b547d88c701881f741afffb.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.1.plutus-d9187a701bca584d89b3560f59a5e472.pdf">here</a>.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 8.9.3]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2024-05-performance-8.9.3</link>
            <guid>2024-05-performance-8.9.3</guid>
            <pubDate>Wed, 13 Mar 2024 09:34:49 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>8.9.1</code> - baseline from a previous mainnet release</li><li><code>8.9.3</code> - the current mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Under value workload, CPU usage increases slightly on <code>8.9.3</code>: 4% for Process, 3% for Mutator and 8% for GC.</li><li>Additionally, Allocation rate and minor GCs increase slightly by 3% each.</li><li>Under Plutus workload only, the GC live dataset increases by 10% or 318MB.</li><li>CPU 85% spans increase by 14% of slot duration under value workload, whereas they shorten by 5% of slot duration under Plutus workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>There are no significant changes to metrics related to block forging.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration improves by 7ms (or 2%) under value workload, and by 4ms (or 3%) under Plutus workload.</li><li>Under Plutus workload, Fetched to sending improves by 2ms (or 5%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload, cluster adoption times exhibit a minor improvement (1%) up to the 80th centile on <code>8.9.3</code>.</li><li>Under Plutus workload, we can observe a minor improvement overall (1% - 2%), whilst full adoption is unchanged.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>The performance changes measured between <code>8.9.3</code> and <code>8.9.1</code> are very minor, with <code>8.9.3</code> improving slightly over <code>8.9.1</code>.</li><li>Mainnnet performance of <code>8.9.3</code> is expected to be akin to <code>8.9.1</code>.</li><li>We have not observed any performance regression being introduced in <code>8.9.3</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.3.value-only-f08f2a51ca31fad16e2cb3e3ad28ee42.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.9.3.plutus-afc8f7bbf2f6f18fb56aede6169fff68.pdf">here</a>.</p><p>NB. The baseline for <code>8.9.1</code> had to be re-established due to changes in the underlying network infrastructure. This means, absolute values may differ from the previous measurements taken from that version.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 8.12.1]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2024-06-performance-8.12.1</link>
            <guid>2024-06-performance-8.12.1</guid>
            <pubDate>Wed, 13 Mar 2024 09:34:49 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>8.9.3</code> - baseline from a previous mainnet release</li><li><code>8.12.1</code> - the current mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Under value workload, CPU usage is improved by 2% - 4%, and by 14% for GCs. Under Plutus workload, CPU usage improves only slightly by 1%.</li><li>Allocation Rate and Minor GCs improve by 5% and 6% - under Plutus workload, there's a slight improvement of 1%.</li><li>RAM usage is reduced by 3%; reduction under Plutus workload is even larger - namely 10%.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Mempool snapshotting improves by 5ms or 7% (3ms or 4% under Plutus workload).</li><li>Adoption time on the block producer improves by 4ms or 6% - under value workload only.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration increases slightly by 6ms or 2% (2ms under Plutus workload).</li><li>Adoption times on the peers improve slightly by 2ms or 3% (1ms under Plutus workload)</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload / full blocks there are no significant changes to cluster adoption times.</li><li>Under Plutus workload / small blocks we can observe a (near-jitter) improvement of 0% - 2% in cluster adoption times.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>The performance changes measured between <code>8.12.1</code> and <code>8.9.3</code> are most distinct in the resource usage footprint - with <code>8.12.1</code> improving over <code>8.9.3</code>.</li><li>On Mainnnet, <code>8.12.1</code> is expected to deliver equal or slightly better performance than <code>8.9.3</code> - as well as lowering the Node's resource usage somewhat in doing so.</li><li>We have not observed any performance regression being introduced in <code>8.12.1</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.12.1.value-only-d18eaee4dbf2ffb1c471c1b82e7ba499.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-8.12.1.plutus-bcf8b8e638f9a7d18710f76fc89e33da.pdf">here</a>.</p><p>NB. The release benchmarks for <code>8.12.1</code> were performed on tag <code>8.12.0-pre</code>. The patch version bump did not include changes relevant to performance; thus, measurements taken on <code>8.12.0-pre</code> remain valid.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 9.0.0]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2024-07-performance-9.0.0</link>
            <guid>2024-07-performance-9.0.0</guid>
            <pubDate>Wed, 13 Mar 2024 09:34:49 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>8.12.1</code> - baseline from a previous mainnet release</li><li><code>9.0.0</code> - the current mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Under value workload Process and Mutator CPU usage are slightly higher on <code>9.0</code> - 7% - 8% (4% each under Plutus workload). GC CPU is increased by 11%, but decreases under Putus workload by 3%.</li><li>Only under value workload, Allocation Rate and Minor GCs increase by 5% and the live GC dataset grows by 3%. Heap size is constant.</li><li>CPU 85% spans are 8% shorter (3% under Plutus workload).</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Mempool Snapshotting and Self Adoption time on the block producer increase very slightly under value workload - 2ms (or 3%) each.</li><li>Under Plutus workload, however, a decrease in Self Adoption time by 2ms (or 4%) is the only significant change in the forging loop.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration is 21ms faster (6%) - 7ms or 5% under Plutus workload.</li><li>Fetched to Sending increases slightly by 3ms (7%) - only under value workload.</li><li>Adoption times on the peers increase slightly by 4ms (5%) - under Plutus workload, however, they are 3ms (6%) faster.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload / full blocks on <code>9.0</code>, we can observe a 4% - 5% improvement of cluster adoption times in the 80th centile and above.</li><li>Under Plutus workload / small blocks, the corresponding improvement is 5% - 6%.</li><li>The main contributing factor is the improvement in Block Fetch duration.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>Network performance clearly improves by ~%5 for 80% to full cluster adoption - independent of workload.</li><li>RAM usage is unchanged on <code>9.0</code>. The slight rise in CPU usage is expected, given improved network performance, and does not pose cause for concern.</li><li>We have not observed any performance regression being introduced in <code>9.0.0.</code>.</li></ul><p>NB. These benchmarks were performed in the Conway ledger era. As such, they do not cover the one-time performance cost of transitioning from Babbage and enabling the new features of the Conway ledger.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-9.0.0.value-only-8ef8064d6bb04de733985e12e9a9e5ea.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-9.0.0.plutus-8385e89a67774584329c93e59dcfe545.pdf">here</a>.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 9.2.0]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2024-09-performance-9.2.0</link>
            <guid>2024-09-performance-9.2.0</guid>
            <pubDate>Wed, 13 Mar 2024 09:34:49 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 2 different versions of <code>cardano-node</code>:</p><ul><li><code>9.1.1</code> - baseline from a previous mainnet release</li><li><code>9.2.0</code> - the current mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Conway era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Under value workload, <code>9.2.0</code> shows an increase by 7% in Process CPU usage.</li><li>Additionally, Allocation Rate and Minor GCs increase by 6% each, while Heap Size remains unchanged.</li><li>Furthermore, CPU 85% spans increase by 10%.</li><li>Under Plutus workload however, there's just one significant observation: a larger portion of the heap is considered live (6% or ~190MB) with the overall Heap Size remaining constant.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>For the forger metrics, we can observe minor (1ms - 2ms) improvements in Ledger Ticking, Mempool Snapshotting and Self Adoption under value workload.</li><li>Under Plutus workload, there are minor (1ms - 2ms) increases in Ledger Ticking and Mempool Snapshotting.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachments) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block Fetch duration has improved by 11ms (or 3%) under value workload.</li><li>Under Plutus workload, peer Adoption times are slightly increased by 2ms (3%).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.  </p><ol><li>Under value workload / full blocks, <code>9.2.0</code> exhibits a slight improvement of 1% - 3% in cluster adoption times.</li><li>Under Plutus workload / small blocks, there's a very minor increase by 1%.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h3><ul><li>We can not detect any perfomance regression in <code>9.2.0</code> compared to <code>9.1.1</code>.</li><li>Under heavy value workload, <code>9.2.0</code> seems to perform work somewhat more eagerly. This would correlate with the slightly increased CPU usage, but also with the improvements in the forging and peer related metrics.</li><li>The clearly increased efficiency of Block Fetch under heavy workload is the main contributing factor to the slight overall network performance improvement.</li></ul><p>NB. These benchmarks were performed using an adjusted, post-Chang hardfork performance baseline to account for added features in the Conway ledger era. Thus, absolute measurements might differ now from those taken using the previous baseline.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>As for publishing such benchmarking results, we are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are still looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-9.2.0.value-only-7c4ead8e5f6e6f965caeb9e7b34cf54d.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/release-9.2.0.plutus-5b1787d5b946bea7b9560731149bb975.pdf">here</a>.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
        <item>
            <title><![CDATA[Benchmarking -- Node 8.7.2]]></title>
            <link>https://updates.cardano.intersectmbo.org/reports/2023-12-performance-8.7.2</link>
            <guid>2023-12-performance-8.7.2</guid>
            <pubDate>Fri, 08 Dec 2023 15:38:20 GMT</pubDate>
            <description><![CDATA[Setup]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>As part of the release benchmarking cycle, we're comparing benchmarking runs for 3 different versions of <code>cardano-node</code>:</p><ul><li><code>8.1.2</code> - the last mainnet release</li><li><code>8.7.0-pre</code> - as an intermediate reference point</li><li><code>8.7.2</code> - the next mainnet release</li></ul><p>For each version, we're gathering various metrics under 2 different workloads:</p><ol><li><em>value-only</em>: Each transaction consumes 2 inputs and creates 2 outputs, changing the UTxO set. This workload produces full blocks (&gt; 80kB) exclusively.</li><li><em>Plutus</em>: Each transaction contains a Plutus script exhausting the per-tx execution budget. This workload produces small blocks (&lt; 3kB) exclusively.</li></ol><p>Benchmarking is performed on a cluster of 52 block producing nodes spread across 3 different AWS regions, interconnected using a static, restricted topology. All runs
were performed in the Babbage era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="observations">Observations<a class="hash-link" href="#observations" title="Direct link to heading">​</a></h2><p>These benchmarks are about evaluating specific corner cases in a constrained environment that allows for reliable reproduction of results; they're not trying to directly recreate the operational conditions on Mainnet.  </p><p>The observations stated refer to the direct comparison between the <code>8.1.2</code> and <code>8.7.2</code> versions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-usage">Resource Usage<a class="hash-link" href="#resource-usage" title="Direct link to heading">​</a></h3><ol><li>Plutus workload, having a lower overall absolute CPU load, exhibits an average increase of 27% in Process CPU usage. Value workload, having a higher overall absolute CPU load, exhibits a near-jitter increase of 1%.</li><li>Allocation rates increase by ~8.9MB/s (value workload) and ~12.6MB/s (Plutus workload).</li><li>Heap sizes increase by 47% - 54%.</li><li>CPU 85% span duration shrinks by ~9.7 slots under value workload, and ~5.8 slots under Pluts workload.</li></ol><p>Caveat: Individual metrics can't be evaluated in isolate; the resource usage profile as a whole provides insight into the system's performance and responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forging-loop">Forging Loop<a class="hash-link" href="#forging-loop" title="Direct link to heading">​</a></h3><ol><li>Block Context Acquisition in the forging loop increases by ~10ms.</li><li>Mempool snapshotting shows an increase by 16ms under value workload; under Plutus workload, it increases by 3ms.</li><li>Ledger ticking improves slightly by 1-2ms.</li></ol><p>The metric <em>'Slot start to announced'</em> (see in attachements) is cumulative, and demonstrates how far into a slot the block producing node first announces the new header.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="peer-propagation">Peer propagation<a class="hash-link" href="#peer-propagation" title="Direct link to heading">​</a></h3><ol><li>Block fetch time increases for full blocks by 9%. For small blocks, it improves by 7%.</li><li>Time to resend a block after fetching increases by 8% for full blocks, whereas it improves by 2% for small blocks.</li><li>Block adoption by a peer takes 12% more time for a full block, but happens faster by 4% for a small block.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-propagation">End-to-end propagation<a class="hash-link" href="#end-to-end-propagation" title="Direct link to heading">​</a></h3><p>This metric encompasses block diffusion and adoption across specific percentages of the benchmarking cluster, with 0.80 adoption meaning adoption on 80% of all cluster nodes.</p><p>The metric exhibits an increase by ~10% across all centiles for full blocks, whereas it improves by 5-6% for small blocks in the higher (80th and above) centiles.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact">Contact<a class="hash-link" href="#contact" title="Direct link to heading">​</a></h2><p>This is the first time we're publishing, to a wider audience, such benchmarking results. We are aware that more context and detail may be needed with regard to specfic metrics or benchmarking methodology. </p><p>We are looking to gather questions, both general and specific, so that we can provide a suitable FAQ and possibly improve presentation in the future.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="attachments">Attachments<a class="hash-link" href="#attachments" title="Direct link to heading">​</a></h2><p>Full report for <em>value-only workload</em>, PDF downloadable <a target="_blank" href="/assets/files/8.7.1_8.1.2_8.7.0-pre_8.7.1-pre.value-only-fdb343a09234cd47578ec1ec47c4610e.pdf">here</a>.</p><p>Full report for <em>Plutus workload</em>, PDF downloadable <a target="_blank" href="/assets/files/8.7.1_8.1.2_8.7.0-pre_8.7.1-pre.plutus-f38724e73290c55323955558c17f2590.pdf">here</a>.</p><p>The relese benchmarks for <code>8.7.2</code> were performed on tag <code>8.7.1-pre</code>, which features identical <code>cardano-node</code> components.</p>]]></content:encoded>
            <category>benchmarking-reports</category>
        </item>
    </channel>
</rss>